## Superoptimization

The Unix shell has a widely diverse set of uses, runs on vastly different platforms, and accesses data of diverse size and complexity.
Additionally, PaSh has several runtime components that affect (and may degrade) script performance.	
Therefore, any parallelization attempt from PaSh needs to be aware of several constraints to provide the ideal performance.
For example, a short-running one-liner, possibly typed interactively by the user, should not see a slowdown.

To address these challenges, PaSh includes an incremental superoptimization component.
The core of this component is a performance model of the parallel script.
We can start with a very simple model and increase its sophistication as we see fit.

A key goal keep the fast path fast.
An assumption is that shell scripts follow a bi-modal distribution---they are either short-running (say, less than a second) or long-running (more than an hour).
(This is is not entirely true as evidenced by PaSh 1.0's Unix50 evaluation, where scripts range between 10^{-1, 0, 1, 2, 3}.)
Thus a key feature of PaSh's superoptimization it operates incrementally over several distinct stages and thus should work adequately for execution time distributions with multiple modes.

A basic model is the following.
The sequential execution time t<sub>s</sub> is defined as the sum time of all stages of computation.
The parallel execution time t<sub>p</sub> is defined as the time to split data, plus the sum of 
the execution times of all parallelizable stages over _n_ processors, plus the sum of the remaining stages, plus merging data.
It makes sense to parallelize only if t<sub>s</sub> > t<sub>p</sub>.

More sophisticated models will be easier to express in LaTeX, so they are just summarized below.
The model should account for:

* Nodes such as `eager` and `split`, as well as their runtime configurations (cost model is especially important for `eager`, see also Fig. 10 of PaSh 1.0)
* The overhead of running (several stages of) the superoptimizer itself.
* The cost of running the compiler itself (which is related to the length of the script and the `width` factor)
* The cost of running and/or testing a few primitives---for example, using a small part of the input.

For some of the configurations configurations the compiler will need to generate new scripts, so the optimizer needs

The superoptimizer may be able to operate at runtime, by using information generated by observing a fraction of the input while the sequential script is executing.
The input size can be identified approximately at constant time using a system call (without having to pay the linear-time cost of a first `split`).
A simple transformation that wraps all inputs and outputs could be achieved by running a very transformation for many scripts that are "likely" parallelizable.

Other ideas:

* Run PaSh with different `width` configs on test inputs to identify the optimal configurations.

* If (1) the input is not large enough, or if (2) the pipeline/program does not execute for long enough, PaSh should abort.

* Some stages can be kept sequential -- e.g., if a `sort` is followed by `tr`, `tr`, it might make sense avoid a `split` after the (parallel) `sort`.

* For some configurations (say, 2x CPUs), it might not even make sense to parallelize a program. i.e., there are programs (or individual commands) that will give speedup above a certain `width` (lower bound).

In the evaluation, it might make sense to use [radar plots](https://en.wikipedia.org/wiki/Radar_chart) to show the exploration of the configuration space.
